{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def get_missing_value_percentage(data):\n",
    "    temp = (data.isnull().sum()/data.shape[0])*100\n",
    "    temp = temp[temp > 0]\n",
    "    return temp\n",
    "\n",
    "def draw_missing_value_percentage(data):\n",
    "    temp = get_missing_value_percentage(data)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.xticks(rotation = 90)\n",
    "    sns.barplot(temp)\n",
    "    plt.show()\n",
    "\n",
    "def set_na_to_median(data, column_name):\n",
    "    condition = data[column_name].isna()\n",
    "    median = data[column_name].median()\n",
    "    data.loc[condition, column_name] = median\n",
    "\n",
    "def set_na_to_value(data, column_name, value):\n",
    "    condition = data[column_name].isna()\n",
    "    data.loc[condition, column_name] = value\n",
    "\n",
    "def box_scatter(data, columns, hue, figsize=(20,5)):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=len(columns), figsize = figsize, sharey=True)\n",
    "    for i,v in enumerate(columns):\n",
    "        sns.boxplot(data = data, y = v, hue = hue, ax=axes[0][i])\n",
    "        axes[0][i].set_title(v)\n",
    "        sns.scatterplot(data = data, x=range(0, data.shape[0]), y = v, hue = hue, ax=axes[1][i])\n",
    "        axes[1][i].set_title(v)\n",
    "    plt.show()\n",
    "\n",
    "def box_scatter_describe(data, columns, hue, figsize=(20,5)):\n",
    "    box_scatter(data, columns, hue, figsize)\n",
    "    condition = data[hue] == 1\n",
    "    print(data[condition][columns].describe())\n",
    "\n",
    "def box_scatter_compare(old_data, new_data, columns, hue, figsize=(20,5)):\n",
    "    fig, axes = plt.subplots(nrows=4, ncols=len(columns), figsize = figsize, sharey=True)\n",
    "    for i,v in enumerate(columns):\n",
    "        sns.boxplot(data = old_data, y = v, hue = hue, ax=axes[0][i])\n",
    "        sns.scatterplot(data = old_data, x=range(0, old_data.shape[0]), y = v, hue = hue, ax=axes[1][i])\n",
    "        sns.boxplot(data = new_data, y = v, hue = hue, ax=axes[2][i])\n",
    "        sns.scatterplot(data = new_data, x=range(0, new_data.shape[0]), y = v, hue = hue, ax=axes[3][i])\n",
    "    plt.show()\n",
    "\n",
    "def get_quantiled_data(data, column_name, quantile):\n",
    "    condition = data[column_name] < data[column_name].quantile(quantile)    \n",
    "    temp = data[condition]\n",
    "    return temp\n",
    "\n",
    "def get_data_loss(old, new):\n",
    "    print(f'initial shape = {old.shape}')\n",
    "    print(f'shape after cleaning = {new.shape}')\n",
    "    print(f'data loss after cleaning = {round (100*(old.shape[0]-new.shape[0])/old.shape[0])}%')\n",
    "\n",
    "import numpy as np\n",
    "def draw_scree_plot(pca):\n",
    "    var_cumu = np.cumsum(pca.explained_variance_ratio_)\n",
    "    fig = plt.figure(figsize=[12,8])\n",
    "    plt.vlines(x=15, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\n",
    "    plt.hlines(y=0.95, xmax=200, xmin=0, colors=\"g\", linestyles=\"--\")\n",
    "    plt.plot(var_cumu)\n",
    "    plt.ylabel(\"Cumulative variance explained\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in the telecom industry, customers churn(leave the service by a provider)\n",
    "- the telecom industry has collected data for 3 months, there were churns in these 3 months.\n",
    "- as a data scientist, we need to identify the patterns and reasons of these churns using ML modelling techniques\n",
    "- the success of the model depends on the accuracy of predictions on the test(unseen) data. If we are able to predict the churn correctly for a reasonable number of customers in the unseen data, we can say which features impact the churn and what action items can be taken to reduce the churn(retain the customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. First look at the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loading and looking at the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data_dictionary = pd.read_csv('data_dictionary.csv')\n",
    "inp0 = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Column comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inp0\n",
    "# a lot of columns have _6, _7, _8 in them, let's drop the suffix to understand what columns exist\n",
    "columns = data.columns\n",
    "columns = columns.str.replace('_6', '')\n",
    "columns = columns.str.replace('_7', '')\n",
    "columns = columns.str.replace('_8', '')\n",
    "print(columns.unique().shape)\n",
    "print(columns.unique().sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inp0\n",
    "# direction of calls => ic, og\n",
    "# type of calls => loc, std, isd, roam, spl, others\n",
    "# call between operators => within network(t2t, t2f, t2c), outside network(t2m, t2o)\n",
    "# mobile network band => 2g, 3g\n",
    "# loyalty related => aon\n",
    "# data usage related => vol_2g, vol_3g\n",
    "# calling usage related => mou => about 90 columns related to mou\n",
    "# money related => arpu, rech_amt\n",
    "# count related => rech_num, total_rech_data, count_rech_2g, count_rech_3g\n",
    "# other parameters => fb_user\n",
    "columns_to_select_1 = data.columns.str.contains('vbc')\n",
    "columns_to_select_2 = data.columns.str.contains('')\n",
    "\n",
    "data.loc[:, columns_to_select_1 & columns_to_select_2].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- there are about 172 columns, about 70k rows\n",
    "- the columns can be understood as follows\n",
    "    - direction of calls => ic, og\n",
    "    - type of calls => loc, std, isd, roam, spl, others\n",
    "    - call between operators => within network(t2t, t2f, t2c), outside network(t2m, t2o)\n",
    "    - mobile network band => 2g, 3g\n",
    "    - loyalty related => aon\n",
    "    - data usage related => vol_2g, vol_3g\n",
    "    - calling usage related => mou => about 90 columns related to mou\n",
    "    - money related => arpu, rech_amt\n",
    "    - type of recharges => nightly, sachet, monthly\n",
    "    - count related => rech_num, total_rech_data, count_rech_2g, count_rech_3g\n",
    "    - other parameters => fb_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparation, Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 set id as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp00 = inp0.set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Removal of columns with single value (one value and NaNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inp00\n",
    "temp = data.nunique() # gives unique count of all the columns\n",
    "unique_condition_1 = temp == 1 # which columns have just 1 unique value\n",
    "temp = temp[unique_condition_1]\n",
    "\n",
    "# verify single value\n",
    "print(temp)\n",
    "data[temp.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inp00\n",
    "data['circle_id'].unique() # contains on 109, can be removed\n",
    "data = data.drop(columns=['circle_id'])\n",
    "\n",
    "data['loc_og_t2o_mou'].value_counts() # about 69k are 0 and remaining are nan, can be dropped\n",
    "data = data.drop(columns=['loc_og_t2o_mou'])\n",
    "\n",
    "data['std_og_t2o_mou'].value_counts() # about 69k are 0 and remaining are nan, can be dropped\n",
    "data = data.drop(columns=['std_og_t2o_mou'])\n",
    "\n",
    "data['loc_ic_t2o_mou'].value_counts() # about 69k are 0 and remaining are nan, can be dropped\n",
    "data = data.drop(columns=['loc_ic_t2o_mou'])\n",
    "\n",
    "data['std_og_t2c_mou_6'].value_counts() # about 67k are 0 and remaining are nan, can be dropped\n",
    "data = data.drop(columns=['std_og_t2c_mou_6'])\n",
    "\n",
    "data['std_og_t2c_mou_7'].value_counts() # about 67k are 0 and remaining are nan, can be dropped\n",
    "data = data.drop(columns=['std_og_t2c_mou_7'])\n",
    "\n",
    "data['std_og_t2c_mou_8'].value_counts() # about 66k are 0 and remaining are nan, can be dropped\n",
    "data = data.drop(columns=['std_og_t2c_mou_8'])\n",
    "\n",
    "data['std_ic_t2o_mou_6'].value_counts() # about 67k are 0 and remaining are nan, can be dropped\n",
    "data = data.drop(columns=['std_ic_t2o_mou_6'])\n",
    "\n",
    "data['std_ic_t2o_mou_7'].value_counts() # about 67k are 0 and remaining are nan, can be dropped\n",
    "data = data.drop(columns=['std_ic_t2o_mou_7'])\n",
    "\n",
    "data['std_ic_t2o_mou_8'].value_counts() # about 66k are 0 and remaining are nan, can be dropped\n",
    "data = data.drop(columns=['std_ic_t2o_mou_8'])\n",
    "\n",
    "data['last_date_of_month_6'].value_counts() # all of them have same value\n",
    "data = data.drop(columns=['last_date_of_month_6'])\n",
    "\n",
    "data['last_date_of_month_7'].value_counts() # about 69k have same value, remaining few are nan\n",
    "data = data.drop(columns=['last_date_of_month_7'])\n",
    "\n",
    "data['last_date_of_month_8'].value_counts() # about 69k have same value, remaining few are nan\n",
    "data = data.drop(columns=['last_date_of_month_8'])\n",
    "\n",
    "inp1 = data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Observation\n",
    "- about 13 columns are dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Change data type of datetime columns from object to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inp1\n",
    "data.select_dtypes(include=['object'])\n",
    "data['date_of_last_rech_6'] = pd.to_datetime(data['date_of_last_rech_6'])\n",
    "data['date_of_last_rech_7'] = pd.to_datetime(data['date_of_last_rech_7'])\n",
    "data['date_of_last_rech_8'] = pd.to_datetime(data['date_of_last_rech_8'])\n",
    "data['date_of_last_rech_data_6'] = pd.to_datetime(data['date_of_last_rech_data_6'])\n",
    "data['date_of_last_rech_data_7'] = pd.to_datetime(data['date_of_last_rech_data_7'])\n",
    "data['date_of_last_rech_data_8'] = pd.to_datetime(data['date_of_last_rech_data_8'])\n",
    "\n",
    "inp2 = data # the changes above are by reference, but just to be sure that in case we make a copy, inp1 is still updated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Missing value treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_missing_value_percentage(inp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing value imputation\n",
    "data = inp2\n",
    "\n",
    "# treat all mou columns to set na values to median\n",
    "temp = get_missing_value_percentage(data)\n",
    "mou_columns_with_na = temp.index[temp.index.str.contains('mou')]\n",
    "[set_na_to_median(data, column) for column in mou_columns_with_na]\n",
    "\n",
    "# treat all og_others, ic_others to set na values to median\n",
    "[set_na_to_median(data, column) for column in ['og_others_6', 'og_others_7', 'og_others_8','ic_others_6', 'ic_others_7', 'ic_others_8']]\n",
    "\n",
    "# treat all date_of_last_rech to set na values to median\n",
    "[set_na_to_median(data, column) for column in ['date_of_last_rech_6', 'date_of_last_rech_7', 'date_of_last_rech_8']]\n",
    "\n",
    "# set date_of_last_rech_data same as date_of_last_rech\n",
    "condition = data['date_of_last_rech_data_6'].isna()\n",
    "data.loc[condition, 'date_of_last_rech_data_6'] = data.loc[condition, 'date_of_last_rech_6']\n",
    "\n",
    "condition = data['date_of_last_rech_data_7'].isna()\n",
    "data.loc[condition, 'date_of_last_rech_data_7'] = data.loc[condition, 'date_of_last_rech_7']\n",
    "\n",
    "condition = data['date_of_last_rech_data_8'].isna()\n",
    "data.loc[condition, 'date_of_last_rech_data_8'] = data.loc[condition, 'date_of_last_rech_8']\n",
    "\n",
    "# treat all arpu_2g/3g to set na values to median\n",
    "[set_na_to_median(data, column) for column in ['arpu_2g_6', 'arpu_2g_7', 'arpu_2g_8', 'arpu_3g_6', 'arpu_3g_7', 'arpu_3g_8']]\n",
    "\n",
    "# treat all counts of recharges to set na values to 0\n",
    "[set_na_to_value(data, column, 0) for column in ['count_rech_2g_6', 'count_rech_2g_7', 'count_rech_2g_8', 'count_rech_3g_6', 'count_rech_3g_7', 'count_rech_3g_8']]\n",
    "\n",
    "# total_rech_data is a sum of count_rech_2g and count_rech_3g, replacing the value will correct the data and impute missing values\n",
    "data['total_rech_data_6'] = data['count_rech_2g_6'] + data['count_rech_3g_6']\n",
    "data['total_rech_data_7'] = data['count_rech_2g_7'] + data['count_rech_3g_7']\n",
    "data['total_rech_data_8'] = data['count_rech_2g_8'] + data['count_rech_3g_8']\n",
    "\n",
    "# if total_rech_data is 0 (no recharge was done), max_rech_data and av_rech_amt_data is also 0\n",
    "condition = data['total_rech_data_6'] == 0\n",
    "data.loc[condition, 'max_rech_data_6'] = 0\n",
    "data.loc[condition, 'av_rech_amt_data_6'] = 0\n",
    "\n",
    "condition = data['total_rech_data_7'] == 0\n",
    "data.loc[condition, 'max_rech_data_7'] = 0\n",
    "data.loc[condition, 'av_rech_amt_data_7'] = 0\n",
    "\n",
    "condition = data['total_rech_data_8'] == 0\n",
    "data.loc[condition, 'max_rech_data_8'] = 0\n",
    "data.loc[condition, 'av_rech_amt_data_8'] = 0\n",
    "\n",
    "# assuming fb_user, night_pck_user to be 0 if information isn't available and mode is NaN\n",
    "[set_na_to_value(data, column, 0) for column in ['fb_user_6', 'fb_user_7', 'fb_user_8','night_pck_user_6', 'night_pck_user_7', 'night_pck_user_8']]\n",
    "\n",
    "draw_missing_value_percentage(data)\n",
    "inp3 = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Observation\n",
    "- we have done some logical missing value imputations, we should validate the data with some sanity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Removal of empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inp3\n",
    "threshold = 1\n",
    "temp = data.isnull().mean(axis=1)*100 > threshold\n",
    "print(f'% rows missing greater than {threshold}% of data = {round(data[temp].shape[0]*100/data.shape[0])}%')\n",
    "print(data[temp].shape)\n",
    "draw_missing_value_percentage(data[~temp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Observation\n",
    "- we don't need to remove rows as there is no missing value now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Outlier treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (30, 5))\n",
    "plt.xticks(rotation = 90)\n",
    "columns_to_remove = []\n",
    "columns_to_remove.extend(inp3.columns[inp3.columns.str.contains('fb_user')])\n",
    "columns_to_remove.extend(inp3.columns[inp3.columns.str.contains('night_pck_user')])\n",
    "columns_to_remove.extend(inp3.columns[inp3.columns.str.contains('monthly')])\n",
    "columns_to_remove.extend(inp3.columns[inp3.columns.str.contains('sachet')])\n",
    "columns_to_remove.extend(inp3.columns[inp3.columns.str.contains('date')])\n",
    "columns_to_remove.extend(inp3.columns[inp3.columns.str.contains('count')])\n",
    "columns_to_remove.extend(inp3.columns[inp3.columns.str.contains('rech_data')])\n",
    "columns_to_remove.extend(inp3.columns[inp3.columns.str.contains('rech_num')])\n",
    "\n",
    "# columns_to_remove.extend(inp3.columns[inp3.columns.str.contains('arpu')])\n",
    "# columns_to_remove.extend(inp3.columns[inp3.columns.str.contains('mou')])\n",
    "# columns_to_remove.extend(inp3.columns[inp3.columns.str.contains('vol')])\n",
    "# columns_to_remove.extend(inp3.columns[inp3.columns.str.contains('vbc')])\n",
    "# columns_to_remove.extend(inp3.columns[inp3.columns.str.contains('aon')])\n",
    "# columns_to_remove.extend(inp3.columns[inp3.columns.str.contains('amt')])\n",
    "# columns_to_remove.extend(inp3.columns[inp3.columns.str.contains('others')])\n",
    "sns.barplot(inp3.drop(columns = columns_to_remove).nunique().sort_values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "def automated_outlier_treatment(data, exclude=[], method='iqr', threshold=3, type='cap'):\n",
    "    numerical_cols = data.select_dtypes(include=['number']).columns\n",
    "    for col in numerical_cols:\n",
    "        if col in exclude:\n",
    "            continue\n",
    "        if method == 'iqr':\n",
    "            Q1 = data[col].quantile(0.01)\n",
    "            Q3 = data[col].quantile(0.99)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            if(data[col].min() < 0):\n",
    "                lower_bound = lower_bound\n",
    "            else:\n",
    "                lower_bound = 0\n",
    "            \n",
    "            if type == 'cap':\n",
    "                data[col] = data[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "            elif type == 'rem':\n",
    "                data = data[(data[col] >= lower_bound) & (data[col] <= upper_bound)]\n",
    "            print(f'col = {col}, lower = {lower_bound}, upper = {upper_bound}, shape = {data.shape}')\n",
    "        elif method == 'zscore':\n",
    "            z_scores = zscore(data[col])\n",
    "            if type == 'cap':\n",
    "                data[col] = data[col].where(np.abs(z_scores) <= threshold, data[col].median())\n",
    "            elif type == 'rem':\n",
    "                data = data[(np.abs(z_scores) <= threshold)]\n",
    "    return data\n",
    "\n",
    "inp4 = automated_outlier_treatment(inp3.copy(), columns_to_remove, 'iqr', threshold=10, type='cap')\n",
    "get_data_loss(inp3, inp4) # shouldn't be a data loss since we are capping\n",
    "# box_scatter(inp4, numerical_cols, 'churn_probability', figsize=(1000, 5))\n",
    "included_columns = set(inp3.columns.to_list()) - set(columns_to_remove)\n",
    "# box_scatter_compare(inp3, inp4, included_columns, 'churn_probability', figsize = (600, 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Segmented univariate analysis over 'churn_probability'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 arpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inp4\n",
    "\n",
    "selected_columns = data.columns[data.columns.str.contains('arpu')]\n",
    "box_scatter_describe(data=data, columns=selected_columns, hue='churn_probability', figsize= (50, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 observation\n",
    "- the arpu of people who would churn is shrinking over time closer to 0\n",
    "- downward trend of arpu is an indicator of churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 onnet, offnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inp4\n",
    "\n",
    "selected_columns = []\n",
    "selected_columns.extend(data.columns[data.columns.str.contains('onnet')])\n",
    "selected_columns.extend(data.columns[data.columns.str.contains('offnet')])\n",
    "print(selected_columns)\n",
    "box_scatter_describe(data=data, columns=selected_columns, hue='churn_probability', figsize= (50, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 observation\n",
    "- mou is higher for offnet in general\n",
    "- customers who churned show a downward trend in mou as compared to customers who stayed, for both onnet and offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 loc_ic_mou, loc_og_mou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inp4\n",
    "\n",
    "selected_columns = []\n",
    "selected_columns.extend(data.columns[data.columns.str.contains('loc_ic_mou')])\n",
    "selected_columns.extend(data.columns[data.columns.str.contains('loc_og_mou')])\n",
    "print(selected_columns)\n",
    "box_scatter_describe(data=data, columns=selected_columns, hue='churn_probability', figsize= (50, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 observation\n",
    "- incoming mou is higher than outgoing mou\n",
    "- customers who churned show a downward trend in their average mou for both incoming and outgoing\n",
    "- in case average mou of incoming and outgoing is going down, can be seen as a possibility of churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 std_ic_mou, std_og_mou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inp4\n",
    "\n",
    "selected_columns = []\n",
    "selected_columns.extend(data.columns[data.columns.str.contains('std_ic_mou')])\n",
    "selected_columns.extend(data.columns[data.columns.str.contains('std_og_mou')])\n",
    "print(selected_columns)\n",
    "box_scatter_describe(data=data, columns=selected_columns, hue='churn_probability', figsize= (50, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 observation\n",
    "- there are more outgoing calls than incoming in std\n",
    "- both incoming and outgoing mou has a downword trend for customers who have churned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.5 isd_ic_mou, isd_og_mou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inp4\n",
    "\n",
    "selected_columns = []\n",
    "selected_columns.extend(data.columns[data.columns.str.contains('isd_ic_mou')])\n",
    "selected_columns.extend(data.columns[data.columns.str.contains('isd_og_mou')])\n",
    "print(selected_columns)\n",
    "box_scatter_describe(data=data, columns=selected_columns, hue='churn_probability', figsize= (50, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.5 observation\n",
    "- isd has more incoming that outgoing\n",
    "- average value of mou is trending downwards for both incoming and outgoing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.6 aon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inp4\n",
    "\n",
    "selected_columns = []\n",
    "selected_columns.extend(data.columns[data.columns.str.contains('aon')])\n",
    "selected_columns.append('aon')\n",
    "print(selected_columns)\n",
    "box_scatter_describe(data=data, columns=selected_columns, hue='churn_probability', figsize= (50, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.6 observation\n",
    "- customers having higher age on network usually stay\n",
    "- looking at the median of customers who did not churn, it can be said that customers below 1000 days will have a higher chances of churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.7 rech_amt, rch_amt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inp4\n",
    "\n",
    "selected_columns = []\n",
    "selected_columns.extend(data.columns[data.columns.str.contains('rech_amt')])\n",
    "selected_columns.extend(data.columns[data.columns.str.contains('rch_amt')])\n",
    "print(selected_columns)\n",
    "box_scatter_describe(data=data, columns=selected_columns, hue='churn_probability', figsize= (50, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.7 observation\n",
    "- recharges are going down month over month for customers who churned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.8 recharge counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inp4\n",
    "\n",
    "selected_columns = []\n",
    "selected_columns.extend(data.columns[data.columns.str.contains('rech_num')])\n",
    "selected_columns.extend(data.columns[data.columns.str.contains('total_rech_data')])\n",
    "selected_columns.extend(data.columns[data.columns.str.contains('count_rech_2g')])\n",
    "selected_columns.extend(data.columns[data.columns.str.contains('count_rech_3g')])\n",
    "print(selected_columns)\n",
    "box_scatter_describe(data=data, columns=selected_columns, hue='churn_probability', figsize= (50, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.8 observation\n",
    "- recharge counts are also going down month over month for the customers who churned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.9 data usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inp4\n",
    "\n",
    "selected_columns = []\n",
    "selected_columns.extend(data.columns[data.columns.str.contains('vol')])\n",
    "print(selected_columns)\n",
    "box_scatter_describe(data=data, columns=selected_columns, hue='churn_probability', figsize= (50, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.9 observation\n",
    "- 3g usage is more than 2g\n",
    "- usage of both 3g and 2g is trending downwards month over month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.10 fb user, night pack user, monthly, sachet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inp4\n",
    "\n",
    "selected_columns = []\n",
    "selected_columns.extend(data.columns[data.columns.str.contains('fb_user')])\n",
    "selected_columns.extend(data.columns[data.columns.str.contains('night_pck_user')])\n",
    "selected_columns.extend(data.columns[data.columns.str.contains('monthly')])\n",
    "selected_columns.extend(data.columns[data.columns.str.contains('sachet')])\n",
    "print(selected_columns)\n",
    "box_scatter_describe(data=data, columns=selected_columns, hue='churn_probability', figsize= (50, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.10 observation\n",
    "- count of fb user, night pack user, monthly user, sachet user is going down over months for churned customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.11 local within network (t2t, t2f, t2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inp4.copy()\n",
    "\n",
    "data['loc_ic_withinnetwork_mou_6'] = data['loc_ic_t2t_mou_6'] + data['loc_ic_t2f_mou_6']\n",
    "data['loc_ic_withinnetwork_mou_7'] = data['loc_ic_t2t_mou_7'] + data['loc_ic_t2f_mou_7']\n",
    "data['loc_ic_withinnetwork_mou_8'] = data['loc_ic_t2t_mou_8'] + data['loc_ic_t2f_mou_8']\n",
    "\n",
    "data['loc_og_withinnetwork_mou_6'] = data['loc_og_t2t_mou_6'] + data['loc_og_t2f_mou_6'] + data['loc_og_t2c_mou_6']\n",
    "data['loc_og_withinnetwork_mou_7'] = data['loc_og_t2t_mou_7'] + data['loc_og_t2f_mou_7'] + data['loc_og_t2c_mou_7']\n",
    "data['loc_og_withinnetwork_mou_8'] = data['loc_og_t2t_mou_8'] + data['loc_og_t2f_mou_8'] + data['loc_og_t2c_mou_8']\n",
    "\n",
    "selected_columns = []\n",
    "selected_columns.extend(data.columns[data.columns.str.contains('withinnetwork')])\n",
    "print(selected_columns)\n",
    "box_scatter_describe(data=data, columns=selected_columns, hue='churn_probability', figsize= (50, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.11 observation\n",
    "- both incoming and outgoing within local network are trending downwards for churned customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.11.1 local outside network t2m t2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inp4\n",
    "\n",
    "selected_columns = []\n",
    "selected_columns.extend(data.columns[data.columns.str.contains('loc_ic_t2m')])\n",
    "selected_columns.extend(data.columns[data.columns.str.contains('loc_og_t2m')])\n",
    "print(selected_columns)\n",
    "box_scatter_describe(data=data, columns=selected_columns, hue='churn_probability', figsize= (50, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.11.1 observation\n",
    "- in general the incoming and outgoing from/to other operators is high for both churned and non churned customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.12 std within network (t2t, t2f, t2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inp4.copy()\n",
    "\n",
    "data['std_ic_withinnetwork_mou_6'] = data['std_ic_t2t_mou_6'] + data['std_ic_t2f_mou_6']\n",
    "data['std_ic_withinnetwork_mou_7'] = data['std_ic_t2t_mou_7'] + data['std_ic_t2f_mou_7']\n",
    "data['std_ic_withinnetwork_mou_8'] = data['std_ic_t2t_mou_8'] + data['std_ic_t2f_mou_8']\n",
    "\n",
    "data['std_og_withinnetwork_mou_6'] = data['std_og_t2t_mou_6'] + data['std_og_t2f_mou_6']\n",
    "data['std_og_withinnetwork_mou_7'] = data['std_og_t2t_mou_7'] + data['std_og_t2f_mou_7']\n",
    "data['std_og_withinnetwork_mou_8'] = data['std_og_t2t_mou_8'] + data['std_og_t2f_mou_8']\n",
    "\n",
    "selected_columns = []\n",
    "selected_columns.extend(data.columns[data.columns.str.contains('withinnetwork')])\n",
    "print(selected_columns)\n",
    "box_scatter_describe(data=data, columns=selected_columns, hue='churn_probability', figsize= (50, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.12 observation\n",
    "- both incoming and outgoing within std network are trending downwards for churned customers\n",
    "- for std outgoing calls are more than incoming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.13 spl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inp4\n",
    "\n",
    "selected_columns = []\n",
    "selected_columns.extend(data.columns[data.columns.str.contains('spl')])\n",
    "print(selected_columns)\n",
    "box_scatter_describe(data=data, columns=selected_columns, hue='churn_probability', figsize= (50, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.13 observation\n",
    "- spl outgoing is decreasing month over month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Feature engineering and Variable Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Create derived columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inp4\n",
    "\n",
    "data['loc_ic_withinnetwork_mou_6'] = data.apply(lambda x: x['loc_ic_t2t_mou_6'] + x['loc_ic_t2f_mou_6'], axis=1)\n",
    "data['loc_ic_withinnetwork_mou_7'] = data.apply(lambda x: x['loc_ic_t2t_mou_7'] + x['loc_ic_t2f_mou_7'], axis=1)\n",
    "data['loc_ic_withinnetwork_mou_8'] = data.apply(lambda x: x['loc_ic_t2t_mou_8'] + x['loc_ic_t2f_mou_8'], axis=1)\n",
    "\n",
    "data['loc_og_withinnetwork_mou_6'] = data.apply(lambda x: x['loc_og_t2t_mou_6'] + x['loc_og_t2f_mou_6'] + x['loc_og_t2c_mou_6'], axis=1)\n",
    "data['loc_og_withinnetwork_mou_7'] = data.apply(lambda x: x['loc_og_t2t_mou_7'] + x['loc_og_t2f_mou_7'] + x['loc_og_t2c_mou_7'], axis=1)\n",
    "data['loc_og_withinnetwork_mou_8'] = data.apply(lambda x: x['loc_og_t2t_mou_8'] + x['loc_og_t2f_mou_8'] + x['loc_og_t2c_mou_8'], axis=1)\n",
    "\n",
    "data['std_ic_withinnetwork_mou_6'] = data.apply(lambda x: x['std_ic_t2t_mou_6'] + x['std_ic_t2f_mou_6'], axis=1)\n",
    "data['std_ic_withinnetwork_mou_7'] = data.apply(lambda x: x['std_ic_t2t_mou_7'] + x['std_ic_t2f_mou_7'], axis=1)\n",
    "data['std_ic_withinnetwork_mou_8'] = data.apply(lambda x: x['std_ic_t2t_mou_8'] + x['std_ic_t2f_mou_8'], axis=1)\n",
    "\n",
    "data['std_og_withinnetwork_mou_6'] = data.apply(lambda x: x['std_og_t2t_mou_6'] + x['std_og_t2f_mou_6'], axis=1)\n",
    "data['std_og_withinnetwork_mou_7'] = data.apply(lambda x: x['std_og_t2t_mou_7'] + x['std_og_t2f_mou_7'], axis=1)\n",
    "data['std_og_withinnetwork_mou_8'] = data.apply(lambda x: x['std_og_t2t_mou_8'] + x['std_og_t2f_mou_8'], axis=1)\n",
    "\n",
    "datetime_columns = data.columns[data.columns.str.contains('')]\n",
    "data['date_of_last_rech_6'] = data['date_of_last_rech_6'].dt.day\n",
    "data['date_of_last_rech_7'] = data['date_of_last_rech_7'].dt.day\n",
    "data['date_of_last_rech_8'] = data['date_of_last_rech_8'].dt.day\n",
    "data['date_of_last_rech_data_6'] = data['date_of_last_rech_data_6'].dt.day\n",
    "data['date_of_last_rech_data_7'] = data['date_of_last_rech_data_7'].dt.day\n",
    "data['date_of_last_rech_data_8'] = data['date_of_last_rech_data_8'].dt.day\n",
    "\n",
    "inp5 = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(inp5, train_size=0.8, random_state=100)\n",
    "X_train = df_train.drop(columns=['churn_probability'])\n",
    "y_train = df_train['churn_probability']\n",
    "X_test = df_test.drop(columns=['churn_probability'])\n",
    "y_test = df_test['churn_probability']\n",
    "\n",
    "print(f'df_train shape: {df_train.shape}')\n",
    "print(f'df_test shape: {df_test.shape}')\n",
    "print(df_train['churn_probability'].value_counts(normalize=True))\n",
    "print(df_test['churn_probability'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 observation\n",
    "- we didn't have to stratify over churn_probability, the distribution of churn_probability in both train and test data is equally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Scaling numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numerical_columns_to_scale = []\n",
    "df_numerical_columns_to_scale.extend(data.columns[data.columns.str.contains('arpu')])\n",
    "df_numerical_columns_to_scale.extend(data.columns[data.columns.str.contains('onnet')])\n",
    "df_numerical_columns_to_scale.extend(data.columns[data.columns.str.contains('offnet')])\n",
    "df_numerical_columns_to_scale.extend(data.columns[data.columns.str.contains('mou')])\n",
    "df_numerical_columns_to_scale.extend(data.columns[data.columns.str.contains('aon')])\n",
    "df_numerical_columns_to_scale.extend(data.columns[data.columns.str.contains('rech_amt')])\n",
    "df_numerical_columns_to_scale.extend(data.columns[data.columns.str.contains('rch_amt')])\n",
    "df_numerical_columns_to_scale.extend(data.columns[data.columns.str.contains('rech_num')])\n",
    "df_numerical_columns_to_scale.extend(data.columns[data.columns.str.contains('total_rech_data')])\n",
    "df_numerical_columns_to_scale.extend(data.columns[data.columns.str.contains('count_rech_2g')])\n",
    "df_numerical_columns_to_scale.extend(data.columns[data.columns.str.contains('count_rech_3g')])\n",
    "df_numerical_columns_to_scale.extend(data.columns[data.columns.str.contains('vol')])\n",
    "df_numerical_columns_to_scale.extend(data.columns[data.columns.str.contains('monthly')])\n",
    "df_numerical_columns_to_scale.extend(data.columns[data.columns.str.contains('sachet')])\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler().fit(X_train[df_numerical_columns_to_scale])\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "# X_train_scaled = pd.DataFrame(scaler.transform(X_train[df_numerical_columns_to_scale]), columns=df_numerical_columns_to_scale)\n",
    "# X_test_scaled = pd.DataFrame(scaler.transform(X_test[df_numerical_columns_to_scale]), columns=df_numerical_columns_to_scale)\n",
    "# X_train[df_numerical_columns_to_scale] = scaler.transform(X_train[df_numerical_columns_to_scale])\n",
    "# X_test[df_numerical_columns_to_scale] = scaler.transform(X_test[df_numerical_columns_to_scale])\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(X_train), columns = X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
    "# X_train_scaled = X_train\n",
    "# X_test_scaled = X_test\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 transform & scale the unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test = test.set_index('id')\n",
    "print(test.shape)\n",
    "\n",
    "data = test\n",
    "data['circle_id'].unique() # contains on 109, can be removed\n",
    "data = data.drop(columns=['circle_id'])\n",
    "\n",
    "data['loc_og_t2o_mou'].value_counts() # about 69k are 0 and remaining are nan, can be dropped\n",
    "data = data.drop(columns=['loc_og_t2o_mou'])\n",
    "\n",
    "data['std_og_t2o_mou'].value_counts() # about 69k are 0 and remaining are nan, can be dropped\n",
    "data = data.drop(columns=['std_og_t2o_mou'])\n",
    "\n",
    "data['loc_ic_t2o_mou'].value_counts() # about 69k are 0 and remaining are nan, can be dropped\n",
    "data = data.drop(columns=['loc_ic_t2o_mou'])\n",
    "\n",
    "data['std_og_t2c_mou_6'].value_counts() # about 67k are 0 and remaining are nan, can be dropped\n",
    "data = data.drop(columns=['std_og_t2c_mou_6'])\n",
    "\n",
    "data['std_og_t2c_mou_7'].value_counts() # about 67k are 0 and remaining are nan, can be dropped\n",
    "data = data.drop(columns=['std_og_t2c_mou_7'])\n",
    "\n",
    "data['std_og_t2c_mou_8'].value_counts() # about 66k are 0 and remaining are nan, can be dropped\n",
    "data = data.drop(columns=['std_og_t2c_mou_8'])\n",
    "\n",
    "data['std_ic_t2o_mou_6'].value_counts() # about 67k are 0 and remaining are nan, can be dropped\n",
    "data = data.drop(columns=['std_ic_t2o_mou_6'])\n",
    "\n",
    "data['std_ic_t2o_mou_7'].value_counts() # about 67k are 0 and remaining are nan, can be dropped\n",
    "data = data.drop(columns=['std_ic_t2o_mou_7'])\n",
    "\n",
    "data['std_ic_t2o_mou_8'].value_counts() # about 66k are 0 and remaining are nan, can be dropped\n",
    "data = data.drop(columns=['std_ic_t2o_mou_8'])\n",
    "\n",
    "data['last_date_of_month_6'].value_counts() # all of them have same value\n",
    "data = data.drop(columns=['last_date_of_month_6'])\n",
    "\n",
    "data['last_date_of_month_7'].value_counts() # about 69k have same value, remaining few are nan\n",
    "data = data.drop(columns=['last_date_of_month_7'])\n",
    "\n",
    "data['last_date_of_month_8'].value_counts() # about 69k have same value, remaining few are nan\n",
    "data = data.drop(columns=['last_date_of_month_8'])\n",
    "\n",
    "data['date_of_last_rech_6'] = pd.to_datetime(data['date_of_last_rech_6'])\n",
    "data['date_of_last_rech_7'] = pd.to_datetime(data['date_of_last_rech_7'])\n",
    "data['date_of_last_rech_8'] = pd.to_datetime(data['date_of_last_rech_8'])\n",
    "data['date_of_last_rech_data_6'] = pd.to_datetime(data['date_of_last_rech_data_6'])\n",
    "data['date_of_last_rech_data_7'] = pd.to_datetime(data['date_of_last_rech_data_7'])\n",
    "data['date_of_last_rech_data_8'] = pd.to_datetime(data['date_of_last_rech_data_8'])\n",
    "\n",
    "# treat all mou columns to set na values to median\n",
    "temp = get_missing_value_percentage(data)\n",
    "mou_columns_with_na = temp.index[temp.index.str.contains('mou')]\n",
    "[set_na_to_median(data, column) for column in mou_columns_with_na]\n",
    "\n",
    "# treat all og_others, ic_others to set na values to median\n",
    "[set_na_to_median(data, column) for column in ['og_others_6', 'og_others_7', 'og_others_8','ic_others_6', 'ic_others_7', 'ic_others_8']]\n",
    "\n",
    "# treat all date_of_last_rech to set na values to median\n",
    "[set_na_to_median(data, column) for column in ['date_of_last_rech_6', 'date_of_last_rech_7', 'date_of_last_rech_8']]\n",
    "\n",
    "# set date_of_last_rech_data same as date_of_last_rech\n",
    "condition = data['date_of_last_rech_data_6'].isna()\n",
    "data.loc[condition, 'date_of_last_rech_data_6'] = data.loc[condition, 'date_of_last_rech_6']\n",
    "\n",
    "condition = data['date_of_last_rech_data_7'].isna()\n",
    "data.loc[condition, 'date_of_last_rech_data_7'] = data.loc[condition, 'date_of_last_rech_7']\n",
    "\n",
    "condition = data['date_of_last_rech_data_8'].isna()\n",
    "data.loc[condition, 'date_of_last_rech_data_8'] = data.loc[condition, 'date_of_last_rech_8']\n",
    "\n",
    "# treat all arpu_2g/3g to set na values to median\n",
    "[set_na_to_median(data, column) for column in ['arpu_2g_6', 'arpu_2g_7', 'arpu_2g_8', 'arpu_3g_6', 'arpu_3g_7', 'arpu_3g_8']]\n",
    "\n",
    "# treat all counts of recharges to set na values to 0\n",
    "[set_na_to_value(data, column, 0) for column in ['count_rech_2g_6', 'count_rech_2g_7', 'count_rech_2g_8', 'count_rech_3g_6', 'count_rech_3g_7', 'count_rech_3g_8']]\n",
    "\n",
    "# total_rech_data is a sum of count_rech_2g and count_rech_3g, replacing the value will correct the data and impute missing values\n",
    "data['total_rech_data_6'] = data['count_rech_2g_6'] + data['count_rech_3g_6']\n",
    "data['total_rech_data_7'] = data['count_rech_2g_7'] + data['count_rech_3g_7']\n",
    "data['total_rech_data_8'] = data['count_rech_2g_8'] + data['count_rech_3g_8']\n",
    "\n",
    "# if total_rech_data is 0 (no recharge was done), max_rech_data and av_rech_amt_data is also 0\n",
    "condition = data['total_rech_data_6'] == 0\n",
    "data.loc[condition, 'max_rech_data_6'] = 0\n",
    "data.loc[condition, 'av_rech_amt_data_6'] = 0\n",
    "\n",
    "condition = data['total_rech_data_7'] == 0\n",
    "data.loc[condition, 'max_rech_data_7'] = 0\n",
    "data.loc[condition, 'av_rech_amt_data_7'] = 0\n",
    "\n",
    "condition = data['total_rech_data_8'] == 0\n",
    "data.loc[condition, 'max_rech_data_8'] = 0\n",
    "data.loc[condition, 'av_rech_amt_data_8'] = 0\n",
    "\n",
    "# assuming fb_user, night_pck_user to be 0 if information isn't available and mode is NaN\n",
    "[set_na_to_value(data, column, 0) for column in ['fb_user_6', 'fb_user_7', 'fb_user_8','night_pck_user_6', 'night_pck_user_7', 'night_pck_user_8']]\n",
    "\n",
    "data = automated_outlier_treatment(data, columns_to_remove, 'iqr', threshold=10, type='cap')\n",
    "\n",
    "data['loc_ic_withinnetwork_mou_6'] = data.apply(lambda x: x['loc_ic_t2t_mou_6'] + x['loc_ic_t2f_mou_6'], axis=1)\n",
    "data['loc_ic_withinnetwork_mou_7'] = data.apply(lambda x: x['loc_ic_t2t_mou_7'] + x['loc_ic_t2f_mou_7'], axis=1)\n",
    "data['loc_ic_withinnetwork_mou_8'] = data.apply(lambda x: x['loc_ic_t2t_mou_8'] + x['loc_ic_t2f_mou_8'], axis=1)\n",
    "\n",
    "data['loc_og_withinnetwork_mou_6'] = data.apply(lambda x: x['loc_og_t2t_mou_6'] + x['loc_og_t2f_mou_6'] + x['loc_og_t2c_mou_6'], axis=1)\n",
    "data['loc_og_withinnetwork_mou_7'] = data.apply(lambda x: x['loc_og_t2t_mou_7'] + x['loc_og_t2f_mou_7'] + x['loc_og_t2c_mou_7'], axis=1)\n",
    "data['loc_og_withinnetwork_mou_8'] = data.apply(lambda x: x['loc_og_t2t_mou_8'] + x['loc_og_t2f_mou_8'] + x['loc_og_t2c_mou_8'], axis=1)\n",
    "\n",
    "data['std_ic_withinnetwork_mou_6'] = data.apply(lambda x: x['std_ic_t2t_mou_6'] + x['std_ic_t2f_mou_6'], axis=1)\n",
    "data['std_ic_withinnetwork_mou_7'] = data.apply(lambda x: x['std_ic_t2t_mou_7'] + x['std_ic_t2f_mou_7'], axis=1)\n",
    "data['std_ic_withinnetwork_mou_8'] = data.apply(lambda x: x['std_ic_t2t_mou_8'] + x['std_ic_t2f_mou_8'], axis=1)\n",
    "\n",
    "data['std_og_withinnetwork_mou_6'] = data.apply(lambda x: x['std_og_t2t_mou_6'] + x['std_og_t2f_mou_6'], axis=1)\n",
    "data['std_og_withinnetwork_mou_7'] = data.apply(lambda x: x['std_og_t2t_mou_7'] + x['std_og_t2f_mou_7'], axis=1)\n",
    "data['std_og_withinnetwork_mou_8'] = data.apply(lambda x: x['std_og_t2t_mou_8'] + x['std_og_t2f_mou_8'], axis=1)\n",
    "\n",
    "datetime_columns = data.columns[data.columns.str.contains('')]\n",
    "data['date_of_last_rech_6'] = data['date_of_last_rech_6'].dt.day\n",
    "data['date_of_last_rech_7'] = data['date_of_last_rech_7'].dt.day\n",
    "data['date_of_last_rech_8'] = data['date_of_last_rech_8'].dt.day\n",
    "data['date_of_last_rech_data_6'] = data['date_of_last_rech_data_6'].dt.day\n",
    "data['date_of_last_rech_data_7'] = data['date_of_last_rech_data_7'].dt.day\n",
    "data['date_of_last_rech_data_8'] = data['date_of_last_rech_data_8'].dt.day\n",
    "\n",
    "test = data\n",
    "\n",
    "test_scaled = pd.DataFrame(scaler.transform(test), columns=test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 PCA + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(0.95, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "print(X_train_pca.shape)\n",
    "print(X_test_pca.shape)\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "classifier = LogisticRegression(random_state=37)\n",
    "model = classifier.fit(X_train_pca, y_train)\n",
    "y_test_pred_prob = model.predict_proba(X_test_pca)[:, 1]\n",
    "\n",
    "# Model Evaluation\n",
    "accuracy_df = pd.DataFrame(y_test)\n",
    "accuracy_df['pred_prob'] = y_test_pred_prob\n",
    "accuracy_df['pred'] = accuracy_df['pred_prob'].apply(lambda x: 1 if x > 0.45 else 0)\n",
    "print(\"{:2.2}\".format(metrics.roc_auc_score(y_test, y_test_pred_prob)))\n",
    "print(metrics.accuracy_score(accuracy_df['churn_probability'], accuracy_df['pred']))\n",
    "\n",
    "# Prediction on unseen data\n",
    "test_pca = pca.transform(test_scaled)\n",
    "test_pred_prob = model.predict_proba(test_pca)[:, 1]\n",
    "test['pred_prob'] = test_pred_prob\n",
    "test['churn_probability'] = test['pred_prob'].apply(lambda x: 1 if x > 0.45 else 0)\n",
    "\n",
    "import datetime\n",
    "test[['churn_probability']].to_csv(f'output_{datetime.datetime.now()}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 PCA + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(0.95, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "print(X_train_pca.shape)\n",
    "print(X_test_pca.shape)\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "classifier = RandomForestClassifier(random_state=37)\n",
    "model = classifier.fit(X_train_pca, y_train)\n",
    "y_test_pred_prob = model.predict_proba(X_test_pca)[:, 1]\n",
    "\n",
    "# Model Evaluation\n",
    "accuracy_df = pd.DataFrame(y_test)\n",
    "accuracy_df['pred_prob'] = y_test_pred_prob\n",
    "accuracy_df['pred'] = accuracy_df['pred_prob'].apply(lambda x: 1 if x > 0.45 else 0)\n",
    "print(\"{:2.2}\".format(metrics.roc_auc_score(y_test, y_test_pred_prob)))\n",
    "print(metrics.accuracy_score(accuracy_df['churn_probability'], accuracy_df['pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
